{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Generative-Models-for-Classification.\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Generative Models for Classification.</a></div><div class=\"lev1\"><a href=\"#Kernel-Ridge-Regression:-Setup.\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Kernel Ridge Regression: Setup.</a></div><div class=\"lev1\"><a href=\"#Kernel-Ridge-Regression:-Foundation-(18-points)}.\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Kernel Ridge Regression: Foundation (18 points)}.</a></div><div class=\"lev1\"><a href=\"#Random-Forest\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Random Forest</a></div><div class=\"lev1\"><a href=\"#Who-will-vote-for-a-bill?-(18-points)\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Who will vote for a bill? (18 points)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Classification. \n",
    "(18 points) \n",
    "\n",
    "Consider a sample of observations $(Y_i, X_i)$, where $Y_i$ is a class label $1,2,...,G$ and $X_i$ is a vector of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(5 pts.)} Suppose you wish to construct a classifier for this problem by using a ``Bayes classifier'', that is, one that estimates the (posterior) probability $p(Y_i=g|X_i)$ for each $g$, and then chooses the $g$ that maximizes this probability. Describe how \\textit{generative models} approach this problem. (That is, how might you theoretically estimate $p(Y_i=g|X_i)$ for each class $g$?)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach consists on first try to find a good model for the joint probability distribuition of $(Y_i,X:i)$, and from there, when given the $X_i$, apply the Bayes Rule, to say that $P(Y_i|X_i) = \\frac{P(Y_i,X_i)}{P(X_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(5 pts.)} Propose the most non-parametric, assumption-free way you can think of to actually estimate the posterior probability $p(Y_i=g|X_i)$ (for each $g$) by a generative approach. Why might this not be a good idea in practice?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most non-parametric way is to assume that the data you have IS, the true distribution, i.e., that the probability to find an event is exactly the fraction of times you found it in the data. It is not a good idea when the space the data lives in is infinite, or has a cardinality not much smaller than the dataset, since we can find new $X_i$ that we have not encountered bofore, or that have not been proportionately sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(5 pts.)} Explain how LDA, QDA, and Naive Bayes classifiers each tackle this problem. What is each doing, and how does each differ in their assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Naive Bayes:* assumes that the different obtained variables are independent. Specially useful when the $X$ vector is categorical, and the amount of data is not enough to cover all the category product space.\n",
    "\n",
    " *QDA and LDA*: Both LDA and QDA assume that the $X$ vector is generated from a normal distribution, which has different mean and variance for the categories. LDA assumes the covariance matrix to be the same for both categories, and QDA does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(3 pts.)} Propose some guidelines for when someone might prefer LDA, QDA, or Naive Bayes classifiers relative to each other, and then why you might sometimes prefer a non-generative approach such as logit or SVM.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, NB will be preferred for discretized categories, and QDA and LDA for continuous categories. QDA is a generalization of LDA, and therefore a superset. The main reasons to prefer LDA are the simplicity of the computations and the fact that this reduces the probability of overfit\n",
    "\n",
    "Non-generative approaches are necessary when you cannot ensure the  $X$ data to predict follows the same distribution as the $X$ data observed in the training set. One example for that could be testing performed in laboratory (artiffical) conditions, whose distribution need not follow the real-world distribuiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression: Setup. \n",
    "(12 points)}\n",
    "One benefit of kernelized ridge regressions (such as Kernel Regularized Least Squares, KRLS) is that even though it fits highly non-linear functions, it has a closed-form analytical solution. Recall from lecture that KRLS can be formulated as a \\textit{linear} problem with the functional form  $Y=Kc + \\epsilon$:\n",
    "\n",
    "$$  \\begin{bmatrix}\n",
    "    f(X_1)\\\\\n",
    "  f(X_2) \\\\\n",
    "  \\vdots\\\\\n",
    "  f(X_N)\n",
    "  \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix}\n",
    "  k(X_1, X_1) & k(X_1, X_2) & \\cdots &   k(X_1, X_N)\\\\\n",
    "  k(X_2, X_1)& k(X_2, X_2) & \\cdots & k(X_2, X_N) \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  k(X_N, X_1) & K(X_N, X_2) & \\cdots & k(X_N, X_N)\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    " \\begin{bmatrix}\n",
    "  c_1\\\\\n",
    "  c_2 \\\\\n",
    "  \\vdots\\\\\n",
    "  c_N\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "\n",
    "\n",
    "* $f(X)$ is the value at $X$ of the function we are trying to approximate.\n",
    "\n",
    "* $K$ is the \\textit{kernel matrix} whose entries $k(X_i,X_j)$ quantify, in some way, the similarities between the values of the independent variables for two different data observations $i$ and $j$, and\n",
    "\n",
    "* $c = [c_1, \\ldots, c_N]^T$ is the column-vector of choice coefficients which we want to solve for.\n",
    "\n",
    "\n",
    "We want a model that satisfies two main criteria: we want to minimize the\n",
    "prediction error, and we want to also favor simpler functions over\n",
    "more complicated ones. To do this, we use (Tikhonov)\n",
    "Regularization. Skipping over some details, this implies choosing\n",
    "coefficients $c$ to solve the following:\n",
    "\\begin{equation}\n",
    "\\underset{c \\in \\mathbb{R}^N}{\\operatorname{argmin}}\\,(Y-Kc)^{T}(Y-Kc)+\\lambda c^{\\top}Kc\n",
    "\\end{equation}\n",
    "\n",
    "where $c^{\\top}Kc$ is an estimated norm of our function (a measure of its\n",
    "``complexity'') and $\\lambda$ is our choice about how much we penalize\n",
    "complicated functions versus how much we penalize prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(10 pts.)} Find the closed-form solution for $\\hat{c}$, the minimizer of $(Y-Kc)^T (Y-Kc) + \\lambda(c^{\\top}Kc)$, showing your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\\begin{equation}\n",
    "L = \\sum_i \\left(Y_i - \\sum_j K_{ij} c_j \\right)^2 +  \\lambda \\sum_{i,j}c_i K_{i,j} c_j\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "\\partial _ {c_k} L =& \n",
    "2\\sum_i \\left(Y_i - \\sum_j K_{ij} c_j \\right)K_{i,k} +  \\lambda \\sum_{j} (K_{j,k}+K_{k,j}) c_j \n",
    "=\\\\=&\n",
    "2\\sum_i K_{ij} Y_i - 2\\sum_{i,j}K_{i,k} K_{ij} c_j  +  \\lambda \\sum_{j} (K_{j,k}+K_{k,j}) c_j \n",
    "=\\\\=& (2 K^T Y - 2 K^TKc+\\lambda (K+K^T)c)_k\n",
    "\\end{align}\n",
    "\n",
    "Therefore the equaiton to solve is:\n",
    "\n",
    "\\begin{equation}\n",
    "    [2 K^TKc+\\lambda (K+K^T)]c =  2 K^T Y \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(2 pts.)} In general, what is the effect of making $\\lambda$ larger or smaller? What is a reasonable approach to choosing $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger $\\lambda$ will give a smoother final solution than a smaller $\\lambda$. A reasonable approach to choose $\\lambda$ is one such that the operator-norm of the $\\lambda (K+K^T)$ part of the equation is similar to the operator-norm of $2 K^TK$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression: Foundation (18 points)}. \n",
    "\n",
    "We are now going to practice deriving how you would have arrived at the \\ref{KCproblem} to begin with. An important interpretation of kernelization is that it effectively maps our examples $X_i$ to a higher dimensional space, $\\phi(X_i) \\in \\mathcal{R}^P$, and allows us to work with models that are linear in these new features  (i.e. functions of the form $\\phi(X_i)^{\\top}\\theta$ for $\\theta \\in \\mathcal{R}^P)$. Consider a positive semi-definite kernel $k(\\cdot,\\cdot)$ such that $k(X_i,X_j)=\\langle \\phi(X_i),\\phi(X_j)\\rangle$. Now, we will work with linear models in $\\phi(X_i)$, specifically, $Y_i=\\phi(X_i)^{\\top}\\theta + \\epsilon_i$. Your goal is to now fit this model using ridge regression, i.e., find:\n",
    "$ \\underset{\\theta \\in \\mathcal{R}^P}{argmin} \\sum_{i=1}^N (Y_i - \\phi(X_i)^{\\top}\\theta)^2+ \\lambda ||\\theta||^2$  where $||\\theta||^2$ is simple $\\theta^{\\top}\\theta$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{2 pts.)} Show that our model, $Y_i=\\phi(X_i)^{\\top}\\theta + \\epsilon_i$, implies $E[Y_i|X_i]=\\phi(X_i)^{\\top}\\theta$.  State any assumptions you require on $\\epsilon_i$ for this to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be true, by deffinition, if and only if $E[\\epsilon_i] =0$, as, by definition $E[Y_i-\\phi(X_i)^T\\theta|X_i] = E[\\epsilon_i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(8 pts.)} Next, show that in solving this minimization problem, you get $\\theta = \\sum_i c_i \\phi(X_i)$\n",
    "and specify what $c_i$ is equal to in terms of the quantities in the minimization probolem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} Using this result, show that your model for $E[Y_i|X_i]$ becomes $$\\sum_{j=1}^N c_j k(X_i,X_j)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} Show that for such a model, $||\\theta||^2=c^{\\top}K c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "(20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} In a few sentences, describe how ``random forest'' works. What parameter(s) would you tune to control the complexity? In your answer, explain how random forest addresses the problem of high-variance that we would encounter with a single tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} On the course website, you'll find \\texttt{Heart.csv}, a reduced and cleaned version of the South African heart dataset. The key outcome variable is ``AHD'', which is an indicator of heart disease. The other variables are measures thought to predict heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a random sample of 150 observations to be your training dataset, and hold-out the rest as test data. Using the \\texttt{randomForest} package, make a model to predict \\texttt{AHD} using the other variables in the dataset. Use \\texttt{mtry}=3 and \\texttt{ntree=1000}. \n",
    "\n",
    "* \\textit{(4 pts.)} Compute the test error on the provided test set, and report it along side the ``out-of-bag'' error for your model. What does the out-of-bag error mean, and do you think it is a good measure of generalization error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} We now wish to see the effect of varying \\texttt{mtry} on the out-of-bag and the test error. Write a function that will take your data and a value of \\texttt{mtry} as arguments, and report back the out-of-bag error with  $ntree=500$ each time.  Run the function using \\texttt{mtry} ranging from 1 to the number of variables in the dataset. Plot the resulting out-of-bag and test-error (on the same plot) as functions of \\textit{mtry}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* \\textit{(4 pts.)} Finally, the lasso logit performed fairly well on this task in class.  Compare the test error from the original random forest model (i.e. with \\texttt{mtry=3}) to the test error you get with a lasso logit model (use \\texttt{glmnet}, choosing $\\lambda$ by cross-validation using \\texttt{cv.glmnet}).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who will vote for a bill? (18 points)\n",
    "\n",
    "In this problem, we will work with a sample of speech data drawn from the 2005 congressional record. Your task is to build a classifier that predicts whether or not a member of congress will vote for a bill, using their speeches. In the file ``pset4words.RData'', there is a matrix (\\texttt{wordMatrix}) comprised of dummy variables for whether or not a speech fragment contains one of 5,262 of the most frequently used words in the corpus. If you prefer to create your own set of predictors (using bigrams, for example), the raw speech data is in the list object \\texttt{speeches}.  \n",
    "\n",
    "The outcome variable, \\texttt{vote} is a variable that takes a value of 1 if the member voted for the bill and a -1 if the member voted against the bill. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(10 pts.)} Your job is to try at least \\textit{five classification techniques}. At least one must be a support vector machine, and at least one must be an ensemble over the other methods you try, such as a weighted or unweighted average.  For each model, write one sentence or so describing how or why this is a reasonable modeling approach for this problem. It is okay if some of your models are not seemingly ideal for this problem, but in those cases explain why they might not be ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} \\textit{Report a reasonable measure of the test error} for each model in a table. Be sure you do not ``cheat'' and report an error rate that is susceptible to over-fitting. Comment on the behavior of your ensemble estimator relative to the best individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \\textit{(4 pts.)} Once you have good estimates of the test error for each model, comment on any differences you see, and whether there are any systematic reasons you can think of why some types of models may have worked better than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
