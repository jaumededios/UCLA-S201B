
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{ulem} % ulem is needed to support strikethroughs (\sout)
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Third problem set}
    \author{Jaume de Dios Pont}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    



    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{require}\PY{p}{(}reshape\PY{p}{)}
        \PY{k+kn}{require}\PY{p}{(}ggplot2\PY{p}{)}
        \PY{k+kn}{require}\PY{p}{(}MASS\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}censReg\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Loading required package: reshape
Loading required package: ggplot2
Loading required package: MASS
Loading required package: maxLik
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
    \end{Verbatim}

    With the inestimable help of Alexander Chan

    \section{Question 1}\label{question-1}

(25pt + 10pt) In this question, we use the file \texttt{brader.csv}
which contains data from Brader, Valentino and Suhay (2008). The file
includes the following variables for \(n=265\) observations:

\begin{itemize}
\tightlist
\item
  the outcome of interest -- a four-point scale in response to
  \emph{Do you think the number of immigrants from foreign countries should be increased or decreased?}
\item
  tone of the story treatment (positive or negative)
\item
  ethnicity of the featured immigrant treatment (Mexican or Russian)
\item
  respondents' age
\item
  respondents' income
\end{itemize}

Consider the following ordered logit model for an ordered outcome
variable with four levels:
\[ \Pr(Y_i \leq j \mid X_i) \ = \ \frac{\exp(\psi_j - X_i^\top\beta)}
            {1 + \exp(\psi_j - X_i^\top\beta)} \] for \(j = 1,2,3,4\)
and \(i = 1,...,n\) where \(\psi_4=\infty\) and
\(X_i = [{\tt tone}_i \ {\tt eth}_i \ {\tt ppage}_i \ {\tt ppincimp}_i]^\top\)
(i.e.~no intercept).

    \textbf{a) (5pt)} Write down the likelihood function.

    \emph{To simplify the notation, the indices* $i,j,k$ *are used consistently,* $i$ *to iterate over the number of observations,*$j$ *to iterate over the number of outcomes, and* $k$ * to iterate over the predictors.}

The log-likelihood can be easily seen to be:

\[
l = 
\prod_{i=1}^n 
\frac{\exp(\psi_{Y_i} - X_i^\top\beta)}
     {1 + \exp(\psi_{Y_i} - X_i^\top\beta)} -
\frac{\exp(\psi_{Y_i-1} - X_i^\top\beta)}
     {1 + \exp(\psi_{Y_i-1} - X_i^\top\beta)}
\]

To simplify, from here on \(\phi\) will be the inverse link function and
\(\phi'\) its derivative. This allows us to write the above expression
in a more Matricial form as:

\[
\prod_{i=1}^n \sum_{j=1}^4
\tilde M_{i,j}
\phi\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_k\right)
\]

where

\[
\tilde M = MK =  M 
\begin{pmatrix}
  1 &  0 &  0 &  0\\
 -1 &  1 &  0 &  0\\
  0 & -1 &  1 &  0\\
  0 &  0 & -1 &  1\\
\end{pmatrix}
\]

And \(M\) is the OneHot matrix for the \(Y_i\), i.e.~a matrix that has
\(n\) rows, and each row is the row vector with zeros everywhere but at
the position \(j\), where the the observed \(Y\) is \(Y_j\).\\
Below, there is a function that computes \(\tilde M\) in R:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} compute\PYZus{}M \PY{o}{=} \PY{k+kr}{function} \PY{p}{(}Y\PY{p}{,}m\PY{p}{)}\PY{p}{\PYZob{}}
            
            \PY{c+c1}{\PYZsh{}This is just a One Hot encoder:}
            n \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}Y\PY{p}{)}
            M1 \PY{o}{=} \PY{k+kp}{t}\PY{p}{(}\PY{k+kt}{matrix}\PY{p}{(}\PY{k+kp}{rep}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}m\PY{p}{)}\PY{p}{,}n\PY{p}{)}\PY{p}{,}m\PY{p}{)}\PY{p}{)}
            M2 \PY{o}{=} \PY{k+kt}{matrix}\PY{p}{(}\PY{k+kp}{rep}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}Y\PY{p}{)}\PY{p}{,}m\PY{p}{)}\PY{p}{,}n\PY{p}{)}
            M \PY{o}{=} \PY{p}{(}M1\PY{o}{==}M2\PY{p}{)}\PY{l+m}{+0}
            
            \PY{c+c1}{\PYZsh{}Create the matrix K}
            K \PY{o}{=} \PY{k+kp}{diag}\PY{p}{(}m\PY{l+m}{+1}\PY{p}{)}
            K \PY{o}{=} \PY{o}{\PYZhy{}}K\PY{p}{[}\PY{l+m}{1}\PY{o}{:}m\PY{p}{,}\PY{p}{]}\PY{o}{+}K\PY{p}{[}\PY{l+m}{2}\PY{o}{:}\PY{p}{(}m\PY{l+m}{+1}\PY{p}{)}\PY{p}{,}\PY{p}{]}
            K \PY{o}{=} K\PY{p}{[}\PY{l+m}{1}\PY{o}{:}m\PY{p}{,}\PY{l+m}{1}\PY{o}{:}m\PY{l+m}{+1}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{}Return the product}
            M\PY{o}{\PYZpc{}*\PYZpc{}}K
        \PY{p}{\PYZcb{}}
\end{Verbatim}

    In practice, however, we are interested in the log-likelihood, which is:

\[
L = \sum_{i=1}^n \log \sum_{j=1}^4
\tilde M_{i,j}
\phi\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_k\right)
\]

The notation is further simplified if we pre-comute the
\(\phi\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_k\right)\) as
\(\phi_{ij}'\) (and the same for \(\phi_{ij}'\).

Below there is a wrapper in \texttt{R} that automatically defines these
quantities for our function, so that we can use them :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} wrapper \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}\PY{k+kp}{expression}\PY{p}{)}\PY{p}{\PYZob{}}
            \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}
                     phi \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}x\PY{p}{)}\PY{p}{\PYZob{}}\PY{k+kp}{exp}\PY{p}{(}x\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{l+m}{1}\PY{o}{+}\PY{k+kp}{exp}\PY{p}{(}x\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{,}
                     dphi \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}x\PY{p}{)}\PY{p}{\PYZob{}}\PY{k+kp}{exp}\PY{p}{(}x\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{l+m}{1}\PY{o}{+}\PY{k+kp}{exp}\PY{p}{(}x\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{2}\PY{p}{\PYZcb{}}\PY{p}{)}
            \PY{p}{\PYZob{}}
                psi \PY{o}{=} \PY{k+kp}{append}\PY{p}{(}psi\PY{p}{,}\PY{l+m}{10}\PY{p}{)}\PY{p}{;} \PY{c+c1}{\PYZsh{}add the \PYZsq{}infinity\PYZsq{}}
                m \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}psi\PY{p}{)}\PY{p}{;} \PY{c+c1}{\PYZsh{}number of outcomes}
                n \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}Y\PY{p}{)} \PY{c+c1}{\PYZsh{}number of observatons}
                M \PY{o}{=} compute\PYZus{}M\PY{p}{(}Y\PY{p}{,}m\PY{p}{)}\PY{p}{;}
                
                \PY{c+c1}{\PYZsh{} transform beta\PYZus{}j to beta\PYZus{}jk (as in part d)}
                \PY{c+c1}{\PYZsh{} whenever necessary}
                \PY{k+kr}{if}\PY{p}{(}\PY{k+kp}{length}\PY{p}{(}\PY{k+kp}{beta}\PY{p}{)} \PY{o}{==} \PY{k+kp}{ncol}\PY{p}{(}X\PY{p}{)}\PY{p}{)}
                    beta2 \PY{o}{=} \PY{k+kt}{matrix}\PY{p}{(}\PY{k+kp}{rep}\PY{p}{(}\PY{k+kp}{beta}\PY{p}{,}m\PY{p}{)}\PY{p}{,}ncol\PY{o}{=}m\PY{p}{,}byrow\PY{o}{=}\PY{l+m}{0}\PY{p}{)}
                \PY{k+kr}{else} beta2 \PY{o}{=} \PY{k+kp}{beta}
                    
                \PY{c+c1}{\PYZsh{}Compute the linear combination inside `psi`}
                linear \PY{o}{=}  \PY{o}{\PYZhy{}} \PY{p}{(}X\PY{o}{\PYZpc{}*\PYZpc{}}beta2\PY{p}{)}
                origin \PY{o}{=}  \PY{k+kt}{matrix}\PY{p}{(}\PY{k+kp}{rep}\PY{p}{(}psi\PY{p}{,}n\PY{p}{)} \PY{p}{,}nrow \PY{o}{=} n\PY{p}{,} byrow \PY{o}{=} \PY{l+m}{1}\PY{p}{)}
                l \PY{o}{=} origin \PY{o}{+}  linear
                
                 phix \PY{o}{=} phi\PY{p}{(}l\PY{p}{)}\PY{p}{;}   phix\PY{p}{[}\PY{p}{,}m\PY{p}{]}\PY{o}{=}\PY{l+m}{1}\PY{p}{;} \PY{c+c1}{\PYZsh{}impose the \PYZsq{}infinity\PYZsq{}}
                dphix \PY{o}{=} phi\PY{p}{(}l\PY{p}{)}\PY{p}{;}  dphix\PY{p}{[}\PY{p}{,}m\PY{p}{]}\PY{o}{=}\PY{l+m}{0}\PY{p}{;} \PY{c+c1}{\PYZsh{}impose the \PYZsq{}infinity\PYZsq{}}
        
                \PY{k+kp}{expression}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}M\PY{p}{,}phix\PY{p}{,}dphix\PY{p}{)}\PY{p}{;}
            \PY{p}{\PYZcb{}}
        \PY{p}{\PYZcb{}}
\end{Verbatim}

    Now we are ready to define the log-likelihood function immediately using
the abstraction wrapper:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} log\PYZus{}likelihood \PY{o}{=} wrapper\PY{p}{(}
            \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}M\PY{p}{,}phi\PY{p}{,}dphi\PY{p}{)}\PY{p}{\PYZob{}}
                \PY{k+kp}{sum}\PY{p}{(}\PY{k+kp}{log}\PY{p}{(}\PY{k+kp}{rowSums}\PY{p}{(}M\PY{o}{*}phi\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{p}{\PYZcb{}}
        \PY{p}{)}
\end{Verbatim}

    The wrapper function is just an abstaction (see it at the end of the
file, in the annex), that computes the variables \texttt{M},
\texttt{phi}\(=\phi\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_k\right)\),
\texttt{dphi}\(=\phi'\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_k\right)\).
We use it because everything is easier in those variables. When a
function goes trough the ``wrapper'', its arguments change to become
(X,Y,beta,psi), and the wrapper computes M, phi and dphi.

    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  (10pt) Derive the score functions for \(\beta\) and \(\psi_j\).
\end{enumerate}

    To simplify the computations, let
\(\phi_{ij}=\phi(\psi_{j} - X_i^\top\beta)\), and
\(\phi_{ij}'=\phi'(\psi_{j} - X_i^\top\beta)\). Then we will have:

\[
L = \sum_{i=1}^n \log \sum_{j=1}^4
\tilde M_{i,j}
\phi_{i,j}
\]

Since deriveatives and sums commute freely, we can compute the score
easily:

\[
\frac{\partial L}{\partial \beta_k} = -
\sum_{i=1}^n \left (\sum_{j=0}^4 \tilde M_{i,j}\phi_{i,j} \right )^{-1}
\left (\sum_{j=0}^4
\tilde M_{i,j}\phi_{i,j}'\right )  X_{ik}
\]

in \texttt{R} that is:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} beta\PYZus{}score \PY{o}{=}  wrapper\PY{p}{(}
            \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}M\PY{p}{,}phi\PY{p}{,}dphi\PY{p}{)}\PY{p}{\PYZob{}}
                \PY{o}{\PYZhy{}}\PY{p}{(}\PY{k+kp}{rowSums}\PY{p}{(}M\PY{o}{*}dphi\PY{p}{)}\PY{o}{/}\PY{k+kp}{rowSums}\PY{p}{(}M\PY{o}{*}phi\PY{p}{)}\PY{p}{)}\PY{o}{\PYZpc{}*\PYZpc{}}X
            \PY{p}{\PYZcb{}}
        \PY{p}{)}
\end{Verbatim}

    For the \(\psi\), the result is equally immediate:

\[
\frac{\partial L}{\partial \psi_j} = 
\sum_{i=1}^n \left (\sum_{j=0}^4 \tilde M_{i,j}\phi_{i,j} \right )^{-1}
\tilde M_{i,j}\phi_{i,j}'
\]

which in \texttt{R} can be written as:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} psi\PYZus{}score \PY{o}{=}  wrapper\PY{p}{(}
            \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}M\PY{p}{,}phi\PY{p}{,}dphi\PY{p}{)}\PY{p}{\PYZob{}}
                r \PY{o}{=} \PY{p}{(}\PY{k+kp}{rowSums}\PY{p}{(}M\PY{o}{*}phi\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{\PYZhy{}1}\PY{o}{\PYZpc{}*\PYZpc{}}\PY{p}{(}M\PY{o}{*}dphi\PY{p}{)}\PY{p}{)}
                r\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{p}{(}\PY{k+kp}{length}\PY{p}{(}r\PY{p}{)}\PY{l+m}{\PYZhy{}1}\PY{p}{)}\PY{p}{]}
            \PY{p}{\PYZcb{}}
        \PY{p}{)}
\end{Verbatim}

    (10pt) Using (a) and (b), calculate the maximum likelihood estimates of
\(\beta\) and \(\psi_j\) and their standard errors via the
\texttt{optim} function in R. Confirm your results by comparing them to
outputs from the \texttt{polr} function in the \texttt{MASS} package.

    Load the data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} brader \PY{o}{=} read.csv\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Data/brader.csv\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    Prepare a Handler function for \texttt{optim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} likelihood\PYZus{}handler \PY{o}{=} \PY{k+kr}{function} \PY{p}{(}x\PY{p}{,}data\PY{p}{)}\PY{p}{\PYZob{}}
            X \PY{o}{=} data\PY{p}{[[}\PY{l+m}{1}\PY{p}{]]}
            Y \PY{o}{=} data\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
            o \PY{o}{=} \PY{k+kp}{ncol}\PY{p}{(}X\PY{p}{)}
            beta \PY{o}{=} x\PY{p}{[}\PY{l+m}{1}\PY{o}{:}o\PY{p}{]}
            psi \PY{o}{=} x\PY{p}{[}\PY{p}{(}o\PY{l+m}{+1}\PY{p}{)}\PY{o}{:}\PY{k+kp}{length}\PY{p}{(}x\PY{p}{)}\PY{p}{]}
            l \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}psi\PY{p}{)}
            diff\PYZus{}psi \PY{o}{=} psi\PY{p}{[}\PY{l+m}{2}\PY{o}{:}l\PY{p}{]}\PY{o}{\PYZhy{}}psi\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{p}{(}l\PY{l+m}{\PYZhy{}1}\PY{p}{)}\PY{p}{]}
            \PY{k+kr}{if}\PY{p}{(}\PY{k+kp}{min}\PY{p}{(}diff\PYZus{}psi\PY{p}{)}\PY{o}{\PYZlt{}=}\PY{l+m}{0}\PY{p}{)}
                \PY{k+kr}{return} \PY{p}{(}\PY{l+m}{1E5}\PY{p}{)}
                
                
            \PY{k+kr}{return}\PY{p}{(}\PY{o}{\PYZhy{}}log\PYZus{}likelihood\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{)}\PY{p}{)}
        \PY{p}{\PYZcb{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} gradient\PYZus{}handler \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}x\PY{p}{,}data\PY{p}{)}\PY{p}{\PYZob{}}
            X \PY{o}{=} data\PY{p}{[[}\PY{l+m}{1}\PY{p}{]]}
            Y \PY{o}{=} data\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
            o \PY{o}{=} \PY{k+kp}{ncol}\PY{p}{(}X\PY{p}{)}
            beta \PY{o}{=} x\PY{p}{[}\PY{l+m}{1}\PY{o}{:}o\PY{p}{]}
            psi \PY{o}{=} x\PY{p}{[}\PY{p}{(}o\PY{l+m}{+1}\PY{p}{)}\PY{o}{:}\PY{k+kp}{length}\PY{p}{(}x\PY{p}{)}\PY{p}{]}
            l \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}psi\PY{p}{)}
            diff\PYZus{}psi \PY{o}{=} psi\PY{p}{[}\PY{l+m}{2}\PY{o}{:}l\PY{p}{]}\PY{o}{\PYZhy{}}psi\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{p}{(}l\PY{l+m}{\PYZhy{}1}\PY{p}{)}\PY{p}{]}
            \PY{k+kr}{if}\PY{p}{(}\PY{k+kp}{min}\PY{p}{(}diff\PYZus{}psi\PY{p}{)}\PY{o}{\PYZlt{}=}\PY{l+m}{0}\PY{p}{)}
                \PY{k+kr}{return}\PY{p}{(}x\PY{o}{*}\PY{l+m}{0}\PY{p}{)}\PY{p}{;}
            sbeta \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}beta\PYZus{}score\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{)}\PY{p}{)} 
            spsi  \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}psi\PYZus{}score\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{)}\PY{p}{)} 
            \PY{k+kr}{return}\PY{p}{(}\PY{k+kp}{append}\PY{p}{(}sbeta\PY{p}{,}spsi\PY{p}{)}\PY{p}{)}
        \PY{p}{\PYZcb{}}
\end{Verbatim}

    Then we prepare the data as our function needs it

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} Y \PY{o}{=} brader\PY{o}{\PYZdl{}}immigr
         X \PY{o}{=} \PY{k+kp}{data.matrix}\PY{p}{(}brader\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}
         data \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}X\PY{p}{,}Y\PY{p}{)}
         beta \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,} \PY{l+m}{0}\PY{p}{,} \PY{l+m}{0}\PY{p}{,} \PY{l+m}{0}\PY{p}{)}
         psi \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{\PYZhy{}1}\PY{p}{,} \PY{l+m}{0}\PY{p}{,} \PY{l+m}{1}\PY{p}{)}
         betapsi \PY{o}{=} \PY{k+kp}{append}\PY{p}{(}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{)}
\end{Verbatim}

    And we run the optimization

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} r \PY{o}{=} optim\PY{p}{(}betapsi\PY{p}{,}likelihood\PYZus{}handler\PY{p}{,} gr \PY{o}{=} gradient\PYZus{}handler\PY{p}{,} data \PY{o}{=} data\PY{p}{,}
                   hessian \PY{o}{=} \PY{l+m}{1}\PY{p}{,} control \PY{o}{=} \PY{p}{(}reltol \PY{o}{=} \PY{l+m}{1E\PYZhy{}12}\PY{p}{)}\PY{p}{)}
         r\PY{o}{\PYZdl{}}par
         r\PY{o}{\PYZdl{}}value
\end{Verbatim}

    \begin{enumerate*}
\item 0.75057472585271
\item 0.182514633025783
\item 0.00954246157357417
\item 0.00453801085115179
\item -1.64987523554466
\item 0.153712692971585
\item 1.37459144576628
\end{enumerate*}

    
    325.94791194349

    
    Standard Errors:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kp}{diag}\PY{p}{(}\PY{k+kp}{solve}\PY{p}{(}r\PY{o}{\PYZdl{}}hessian\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{.5}
\end{Verbatim}

    \begin{enumerate*}
\item 0.162040401841744
\item 0.160164676752734
\item 0.00530840672161387
\item 0.0230051608556128
\item 0.430797558900244
\item 0.381474494066442
\item 0.370321674525004
\end{enumerate*}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{library}\PY{p}{(}MASS\PY{p}{)}
         
         plr \PY{o}{\PYZlt{}\PYZhy{}} polr\PY{p}{(} \PY{k+kp}{factor}\PY{p}{(}immigr\PY{p}{)} \PY{o}{\PYZti{}} tone \PY{o}{+} eth \PY{o}{+} ppage \PY{o}{+} ppincimp\PY{p}{,} 
                            data \PY{o}{=} brader\PY{p}{,} method \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{logistic\PYZdq{}}\PY{p}{,} Hess \PY{o}{=} \PY{l+m}{1}\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}plr\PY{p}{)}
         \PY{o}{\PYZhy{}}log\PYZus{}likelihood\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}plr\PY{o}{\PYZdl{}}coefficients\PY{p}{,}plr\PY{o}{\PYZdl{}}z\PY{p}{)}
\end{Verbatim}

    
    \begin{verbatim}
Call:
polr(formula = factor(immigr) ~ tone + eth + ppage + ppincimp, 
    data = brader, Hess = 1, method = "logistic")

Coefficients:
            Value Std. Error t value
tone     0.749446   0.230241  3.2550
eth      0.166225   0.227063  0.7321
ppage    0.009345   0.007131  1.3105
ppincimp 0.004149   0.029511  0.1406

Intercepts:
    Value   Std. Error t value
1|2 -1.6671  0.5664    -2.9434
2|3  0.1318  0.5401     0.2441
3|4  1.3535  0.5466     2.4761

Residual Deviance: 651.8892 
AIC: 665.8892 
    \end{verbatim}

    
    325.944618379387

    
    ** d. (10pt) Bonus question. ** The standard ordered logit model is
sometimes called the proportional odds model because it assumes the
effect of \(X_i\) is constant across levels on the odds ratio scale. One
approach to relax this assumption is to allow the coefficients to vary
across levels, i.e.,
\[ \Pr(Y_i \leq j \mid X_i) \ = \ \frac{\exp(\psi_j - X_i^\top\beta_j)}
            {1 + \exp(\psi_j - X_i^\top\beta_j)} \] for \(j = 1,2,3,4\)
and \(i = 1,...,n\) where \(\beta_4=0\) (i.e.~the fourth group is a
reference group), and \(\psi_4=\infty\). For this model, derive the
likelihood and score functions, and use \texttt{optim} to obtain the
maximum likelihood estimates of \(\beta_j\) and \(\psi_j\) and their
standard errors for the \texttt{brader} data.

    A direct modification of the above exercise, substituting \(\beta_k\)
for \(\beta_{k,j}\) works: We will have:

\[
L = \sum_{i=1}^n \log \sum_{j=1}^4
\tilde M_{i,j}
\phi\left(\psi_{j} - \sum_{k=1}^m X_{ik}\beta_{kj}\right)
\]

therefore, since the formula is essentialy the same (and thanks to
\texttt{R} not really caring much about the shape of the objects --
which is nice!), the same \texttt{log\_likelihood} coded above still
works!

    Since deriveatives and sums commute freely, we can compute the score
easily:

\[
\frac{\partial L}{\partial \beta_{kj}} = -
\sum_{i=1}^n \left (\sum_{j=0}^4 \tilde M_{i,j}\phi_{i,j} \right )^{-1}
\tilde M_{i,j}\phi_{i,j}'  X_{ik}
\]

in this case we must modify the function, to create a new function:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} extra\PYZus{}beta\PYZus{}score \PY{o}{=} wrapper\PY{p}{(}
             \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{,}M\PY{p}{,}phi\PY{p}{,}dphi\PY{p}{)}\PY{p}{\PYZob{}}
                 \PY{c+c1}{\PYZsh{}this corresponds to a matrix that has the same elements in every}
                 \PY{c+c1}{\PYZsh{}row, and are the (sum **)\PYZca{}\PYZob{}\PYZhy{}1\PYZcb{} in the formula above}
                 M1 \PY{o}{=} \PY{k+kt}{matrix}\PY{p}{(}\PY{k+kp}{rep}\PY{p}{(}\PY{k+kp}{rowSums}\PY{p}{(}M\PY{o}{*}phi\PY{p}{)}\PY{p}{,}\PY{k+kp}{ncol}\PY{p}{(}M\PY{p}{)}\PY{p}{)}\PY{p}{,}ncol\PY{o}{=}\PY{k+kp}{ncol}\PY{p}{(}M\PY{p}{)}\PY{p}{,}byrow\PY{o}{=}\PY{l+m}{0}\PY{p}{)}
                 \PY{o}{\PYZhy{}}\PY{k+kp}{t}\PY{p}{(}M\PY{o}{*}dphi\PY{o}{/}M1\PY{p}{)}\PY{o}{\PYZpc{}*\PYZpc{}}X
             \PY{p}{\PYZcb{}}
         \PY{p}{)}
\end{Verbatim}

    For the \(\psi\), the result is equally immediate:

\[
\frac{\partial L}{\partial \psi_j} = 
\sum_{i=1}^n \left (\sum_{j=0}^4 \tilde M_{i,j}\phi_{i,j} \right )^{-1}
\tilde M_{i,j}\phi_{i,j}'
\]

As for the likelihood, the same function does the job

    Now the code: Create the initial conditions from the optimization in the
previous case:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} beta0 \PY{o}{=} \PY{k+kp}{rep}\PY{p}{(}plr\PY{o}{\PYZdl{}}coefficients \PY{p}{,} \PY{l+m}{4}\PY{p}{)}
         psi0 \PY{o}{=} plr\PY{o}{\PYZdl{}}zeta
         betapsi0 \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}beta0\PY{p}{,}psi0\PY{p}{)}
\end{Verbatim}

    create a new likelihood handler

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} extra\PYZus{}likelihood\PYZus{}handler \PY{o}{=} \PY{k+kr}{function} \PY{p}{(}x\PY{p}{,}data\PY{p}{)}\PY{p}{\PYZob{}}
             X \PY{o}{=} data\PY{p}{[[}\PY{l+m}{1}\PY{p}{]]}
             Y \PY{o}{=} data\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
             o \PY{o}{=} \PY{k+kp}{ncol}\PY{p}{(}X\PY{p}{)}
             beta \PY{o}{=} \PY{k+kt}{matrix}\PY{p}{(}x\PY{p}{[}\PY{l+m}{1}\PY{o}{:}o\PY{o}{*}\PY{l+m}{4}\PY{p}{]}\PY{p}{,}ncol\PY{o}{=}\PY{l+m}{4}\PY{p}{,} byrow \PY{o}{=} \PY{l+m}{0}\PY{p}{)}
             psi \PY{o}{=} x\PY{p}{[}\PY{p}{(}o\PY{o}{*}\PY{l+m}{4}\PY{l+m}{+1}\PY{p}{)}\PY{o}{:}\PY{k+kp}{length}\PY{p}{(}x\PY{p}{)}\PY{p}{]}
             l \PY{o}{=} \PY{k+kp}{length}\PY{p}{(}psi\PY{p}{)}
             diff\PYZus{}psi \PY{o}{=} psi\PY{p}{[}\PY{l+m}{2}\PY{o}{:}l\PY{p}{]}\PY{o}{\PYZhy{}}psi\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{p}{(}l\PY{l+m}{\PYZhy{}1}\PY{p}{)}\PY{p}{]}
             \PY{k+kr}{if}\PY{p}{(}\PY{k+kp}{min}\PY{p}{(}diff\PYZus{}psi\PY{p}{)}\PY{o}{\PYZlt{}=}\PY{l+m}{0}\PY{p}{)}
                 \PY{k+kr}{return} \PY{p}{(}\PY{l+m}{1E5}\PY{p}{)}
                 
                 
             \PY{k+kr}{return}\PY{p}{(}\PY{o}{\PYZhy{}}log\PYZus{}likelihood\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}psi\PY{p}{)}\PY{p}{)}
         \PY{p}{\PYZcb{}}
             
         r \PY{o}{=} optim\PY{p}{(}betapsi0\PY{p}{,}extra\PYZus{}likelihood\PYZus{}handler\PY{p}{,}  data \PY{o}{=} data\PY{p}{,}
                   hessian \PY{o}{=} \PY{l+m}{1}\PY{p}{,} control \PY{o}{=} \PY{p}{(}reltol \PY{o}{=} \PY{l+m}{1E\PYZhy{}12}\PY{p}{)}\PY{p}{)}
         
         \PY{k+kt}{matrix}\PY{p}{(}r\PY{o}{\PYZdl{}}par\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{16}\PY{p}{]}\PY{p}{,}byrow\PY{o}{=}\PY{l+m}{0}\PY{p}{,}ncol\PY{o}{=}\PY{l+m}{4}\PY{p}{)}
         r\PY{o}{\PYZdl{}}par\PY{p}{[}\PY{l+m}{17}\PY{o}{:}\PY{l+m}{19}\PY{p}{]}
\end{Verbatim}

    \begin{tabular}{llll}
      0.6943061   &  0.63885579  & 0.72700705   &  0.835971250\\
     -0.3021024   &  0.14222286  & 0.10165316   & -0.060505633\\
     -0.1915581   & -0.02427104  & 0.93475166   & -0.377267011\\
      0.7014210   & -0.03038976  & 0.01123405   &  0.007652073\\
\end{tabular}

    
    \begin{description*}
\item[1\textbackslash{}textbar\{\}2] -1.66549494997964
\item[2\textbackslash{}textbar\{\}3] 0.152774935941614
\item[3\textbackslash{}textbar\{\}4] 1.36057070649973
\end{description*}

    
    \section{Question 2}\label{question-2}

    (30 pt.) Understanding the basics of maximum likelihood makes it easier
to learn a wide range of models. Here you will learn about a model not
covered in class. The \{\it tobit model\} is often used to model a
censored outcome variable, where we only observe values of the outcome
above a certain threshold \(\tau\) while values below that threshold
take on a single value, say 0. A classic example is labor market
participation: you only observe a worker's wage if their potential
earnings is high enough to be over minimium wage or otherwise
employable.

Consider the case where \(\tau=0\). In this case, the model can be
written as follows.

\begin{eqnarray*}
    Y_i & = & \left\{ \begin{array}{ll}
                    Y_i^\ast & {\rm if} \quad Y_i^\ast > 0 \\
                    0 & {\rm if} \quad Y_i^\ast \le 0 \end{array} \right.
                    \quad {\rm where} \quad Y_i^\ast = X_i^\top \beta + \epsilon_i,
  \end{eqnarray*}

where \(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\). Note that \(Y_i\) is
the observed outcome variable and \(Y_i^\ast\) is the underlying latent
variable, which is not (always) observable.

    \textbf{a.}(10pt) Derive the likelihood and log-likelihood functions of
the model for a simple random sample of size \(N\).

    We will (for a moment) forget that log-likelihood is in fact terribly
ill posed for variables that are a mixture of a continuous and a
discrete variable, and proceed as if we didn't know that:

We have that:

\[
P(Y^*_k > 0) = P(Y_k > 0) = \Phi\left(\frac{x^T\beta}{\sigma}\right)
\]

Moreover, when \(Y_k>0\) \[
\rho(Y^*_k|Y^*_k>0) = \rho(Y_k|Y_k>0) = (P(Y_k>0))^{-1} \phi\left (\frac{x^T\beta-Y_k}{\sigma}\right )
\]

For the other case we have:

\[
P(Y_k = 0) = P(Y^*_k < 0) = 1-\Phi\left(\frac{x^T\beta}{\sigma}\right)
\]

Therefore the likelihood is

\[
L(Y_i) = 
\left\{
\begin{array}{lr}
        1-\Phi\left(\frac{X_i^T\beta}{\sigma}\right), &  Y = 0\\
        \phi\left (\frac{X_i^T\beta-Y_i}{\sigma}\right ), & Y > 0 \\
\end{array}
\right.
\]

We will pretend what we did above is right for the sake of the exercise,
but a careful second look shows that we have brutally mixed
probabilities and probability densities in an almost non-sensical way.
Moreover note that the density is not even continuous, so any attempt to
apply any of the theorems done at class is not justified (at least in
the way we did them at class).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} tobit\PYZus{}log\PYZus{}likelihood \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}sigma\PY{p}{)}\PY{p}{\PYZob{}}
             x \PY{o}{=} X\PY{o}{\PYZpc{}*\PYZpc{}}\PY{k+kp}{beta}\PY{o}{/}sigma
             PN \PY{o}{=} \PY{l+m}{1}\PY{o}{\PYZhy{}}pnorm\PY{p}{(}x\PY{p}{)}
             PP \PY{o}{=}  dnorm\PY{p}{(}x\PY{p}{)}
             PF \PY{o}{=} PN\PY{p}{;}
             PF\PY{p}{[}Y\PY{o}{\PYZgt{}}\PY{l+m}{0}\PY{p}{]} \PY{o}{=} PP
             \PY{k+kp}{sum}\PY{p}{(}\PY{k+kp}{log}\PY{p}{(}PF\PY{p}{)}\PY{p}{)}
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    \textbf{b.} (10pt) A natural quantity of interest for this model is the
expected value of the outcome variable conditional on a set of predictor
levels, i.e., \(E(Y_i \mid X_i=x)\). Prove that the exact expression for
this quantity is the following.

\begin{equation*}
    E(Y_i \mid X_i=x) =  \Phi\left(\frac{x^\top \beta}{\sigma}\right)x^\top\beta + \sigma \phi\left(\frac{x^\top \beta}{\sigma}\right),
  \end{equation*}

where \(\phi(\cdot)\) and \(\Phi(\cdot)\) denote the PDF and CDF of a
standard normal random variable, respectively. \textbackslash{}
\{\bf Hint:\} You may find the following property of the standard normal
PDF helpful: \(\partial\phi(z)/\partial z = -z\cdot \phi(z)\).

    The expectation can be computed as the following integral:

\[
E(Y_i) =\int_{0}^\infty  \frac{d}{dy} \Phi\left (\frac{X_i^T\beta-y}{\sigma}\right ) y dy 
\]

Where we neglected the case where \(Y_i=0\)

In order to compute it, we will perform a change of variables, with
\(z = \frac{X_i^T\beta-y}{\sigma}\)

\begin{align*}
E(Y_i)=
\int_{-\infty}^{\frac{X_i^T\beta}{\sigma}}
      \frac{d}{dz} \Phi\left (z\right ) (X_i^T\beta - z\sigma)dy = &
%
%
\Phi\left(\frac{X_i^T\beta}{\sigma}\right)X_i^T\beta 
+ \sigma \int_{-\infty}^{\frac{X_i^T\beta}{\sigma}} -z \frac{d}{dz} \Phi(z) dz = \\ = &
\Phi\left(\frac{X_i^T\beta}{\sigma}\right)X_i^T\beta 
+ \sigma \int_{-\infty}^{\frac{X_i^T\beta}{\sigma}} \frac{d}{dz} \phi(z) dz = \\ = &
\Phi\left(\frac{X_i^T\beta}{\sigma}\right)X_i^T\beta 
+\sigma \phi\left(\frac{X_i^T\beta}{\sigma}\right)
\end{align*}

    \emph{c.} (10pt) Nielsen (2013, \{\it ISQ\}) investigates the impact of
human rights violations on the flows of foreign aid from donor states to
recipient states. The file \{\tt nielsenaid.csv\} contains a small
subset of the original dataset and consists of the following variables
measured for each donor-recipient dyad:

\begin{itemize}
\tightlist
\item
  \texttt{lneconaid} --- log of economic aid commitments in U.S. dollars
  in 2000
\item
  \texttt{l.physint} --- physical integrity violations index
  (e.g.~toture) in 1999
\item
  \texttt{l.polity2} --- Polity IV democracy score in 1999
\item
  \texttt{l.lneconaid} --- one-year lagged value of \texttt{lneconaid}
\item
  \texttt{l.lnrgdp} --- log GDP per capita in 1999 candidate
\item
  \texttt{l.lnpop} -- log population in 1999
\item
  \texttt{colony} --- indicator of former colony
\item
  \texttt{recep} --- recipient country name degree
\item
  \texttt{donor} --- donor country name
\end{itemize}

Fit the above tobit model, where

\begin{eqnarray*}
Y_i & = & {\tt lneconaid}, \\
X_i & = & [ { {\tt l.physint}, {\tt l.polity2}, {\tt l.lneconaid}, {\tt l.lnrgdp}, {\tt l.lnpop}, {\tt colony} } ]^\top,
\end{eqnarray*}

and estimate \(E[Y_i \mid X_i=x]\) and its asymptotic 95\% confidence
interval, where \(x\) equals the medians for all predictors except
\{\tt colony\}, and the mode for \{\tt colony\}. (For this question, you
may ignore possible error correlation across observations, such as
clustering. You may use a canned procedure to obtain the MLE of
\(\beta\) and \(\sigma\) as well as their sampling variance matrix; but
for \{\bf extra credit\}, calculate those via \{\tt optim\} on your
own.)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} df \PY{o}{=} read.csv\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Data/nielsenaid.csv\PYZsq{}}\PY{p}{)}
         model \PY{o}{=} censReg\PY{p}{(} lneconaid \PY{o}{\PYZti{}} l.physint\PY{o}{+}l.polity2\PY{o}{+}l.lneconaid\PY{o}{+}l.lnrgdp\PY{o}{+}l.lnpop\PY{o}{+}colony\PY{p}{,} 
                          left \PY{o}{=} \PY{l+m}{0}\PY{p}{,} right \PY{o}{=} \PY{k+kc}{Inf}\PY{p}{,} data \PY{o}{=} df\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}model\PY{p}{)}
\end{Verbatim}

    
    \begin{verbatim}

Call:
censReg(formula = lneconaid ~ l.physint + l.polity2 + l.lneconaid + 
    l.lnrgdp + l.lnpop + colony, left = 0, right = Inf, data = df)

Observations:
         Total  Left-censored     Uncensored Right-censored 
          3129           1872           1257              0 

Coefficients:
            Estimate Std. error t value  Pr(> t)    
(Intercept)  0.62113    0.75383   0.824   0.4100    
l.physint    0.07140    0.03844   1.858   0.0632 .  
l.polity2    0.01395    0.01209   1.154   0.2486    
l.lneconaid  1.21494    0.02580  47.095  < 2e-16 ***
l.lnrgdp    -0.52425    0.07526  -6.966 3.27e-12 ***
l.lnpop      0.11769    0.05297   2.222   0.0263 *  
colony      -0.12036    0.36842  -0.327   0.7439    
logSigma     1.08275    0.02182  49.628  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-likelihood: -3879.845 on 8 Df

    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} coef \PY{o}{=}
         \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}l.physint\PY{p}{)}\PY{p}{,}
         \PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}l.polity2\PY{p}{)}\PY{p}{,}
         \PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}l.lneconaid\PY{p}{)}\PY{p}{,}
         \PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}l.lnrgdp\PY{p}{)}\PY{p}{,}
         \PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}l.lnpop\PY{p}{)}\PY{p}{,}
         \PY{p}{(}\PY{k+kp}{mean}\PY{p}{(}df\PY{o}{\PYZdl{}}colony\PY{p}{)}\PY{o}{\PYZlt{}}\PY{l+m}{.5}\PY{p}{)}\PY{l+m}{+0}\PY{p}{)} \PY{c+c1}{\PYZsh{}Mode of colony}
\end{Verbatim}

    Now we compute the value of \(Y*\) and its standard deviation. The
\([1,7]\) is because the last variable is logSigma, which we do not need
to use. The idea is that we first compute the covariance matrix of
\(\hat\beta\), which is the inverse of the hessian, and then the
variance of \(\langle \hat\beta,x\rangle\) is precisely \$ x \^{}T
COV(\hat\beta) x\$

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} Ystar \PY{o}{=} model\PY{o}{\PYZdl{}}estimate\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{7}\PY{p}{]}\PY{o}{\PYZpc{}*\PYZpc{}}coef
         Ystarsd\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}coef\PY{o}{\PYZpc{}*\PYZpc{}}\PY{k+kp}{solve}\PY{p}{(}model\PY{o}{\PYZdl{}}hessian\PY{p}{)}\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{7}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{7}\PY{p}{]}\PY{o}{\PYZpc{}*\PYZpc{}}coef\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{.5}
         Ystar
         Ystarsd
\end{Verbatim}

    \begin{tabular}{l}
     -0.4422193\\
\end{tabular}

    
    \begin{tabular}{l}
     0.3715102\\
\end{tabular}

    
    From this we know that the lower bound of a \(95\%\) confidence interval
must be 0. Therefore, the upper bound can be compyted with the quantile
of the normal (assimptotically).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} Ystar\PY{o}{+}qnorm\PY{p}{(}\PY{l+m}{.95}\PY{p}{)}\PY{o}{*}Ystarsd
\end{Verbatim}

    \begin{tabular}{l}
     0.1688606\\
\end{tabular}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} tobit\PYZus{}likelihood\PYZus{}handler \PY{o}{=} \PY{p}{(}\PY{k+kr}{function}\PY{p}{(}X\PY{p}{,}Y\PY{p}{)}\PY{p}{\PYZob{}}
             \PY{k+kr}{function}\PY{p}{(}betasigma\PY{p}{)}\PY{p}{\PYZob{}}
                 beta \PY{o}{=} betasigma\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{7}\PY{p}{]}
                 sigma \PY{o}{=} betasigma\PY{p}{[}\PY{l+m}{8}\PY{p}{]}
                 \PY{o}{\PYZhy{}}tobit\PYZus{}log\PYZus{}likelihood\PY{p}{(}X\PY{p}{,}Y\PY{p}{,}\PY{k+kp}{beta}\PY{p}{,}sigma\PY{p}{)}
             \PY{p}{\PYZcb{}}
         \PY{p}{\PYZcb{}}
         \PY{p}{)}\PY{p}{(}X\PY{p}{,}Y\PY{p}{)}
         start \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.6}\PY{p}{,}\PY{l+m}{0.07}\PY{p}{,}\PY{l+m}{0.01}\PY{p}{,}\PY{l+m}{1.2}\PY{p}{,}\PY{l+m}{\PYZhy{}0.5}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.12}\PY{p}{,}\PY{l+m}{2.7}\PY{p}{)}
         r \PY{o}{=} optim\PY{p}{(}start\PY{p}{,}tobit\PYZus{}likelihood\PYZus{}handler\PY{p}{,}
                   hessian \PY{o}{=} \PY{l+m}{1}\PY{p}{,} control \PY{o}{=} \PY{p}{(}reltol \PY{o}{=} \PY{l+m}{1E\PYZhy{}12}\PY{p}{)}\PY{p}{)}
         r\PY{o}{\PYZdl{}}par
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        Error in X \%*\% beta: requires numeric/complex matrix/vector arguments
    Traceback:


        1. optim(start, tobit\_likelihood\_handler, hessian = 1, control = (reltol = 1e-12))

        2. (function (par) 
     . fn(par, {\ldots}))(c(0.6, 0.07, 0.01, 1.2, -0.5, 0.1, 0.12, 2.7))

        3. fn(par, {\ldots})

        4. tobit\_log\_likelihood(X, Y, beta, sigma)   \# at line 5 of file <text>

    \end{Verbatim}

    \section{Question 3}\label{question-3}

(20 pt.) Consider a supervised learning problem in which you sample
\(Y_i\) and \(X_i\) from \(p(X,Y)\). You assume an additive noise model
such that: \(Y_i=f(X_i)+\epsilon_i\), and \(E[\epsilon_i|X_i]=0\). We
wish to come up with \(\hat{f}(X)\) that is the best approximation to
\(f(X)\) in some sense.

    \textbf{a} (8pt.) First choose your loss function,
\(L(Y_i,\hat f(X_i))\) to be squared loss. Show that the minimizer of
this loss is found by choosing \(\hat{f}(X_i)=E[Y_i|X_i]\). (It suffices
to show this pointwise, i.e.~conditional on a particular choice of
\(X\)).

    Pointwise we have

\begin{align*}
E(L[Y_i,\hat f(X_I))] = &
E[(Y_i-f(X_i))^2|X_I] =  \\ = &
E[(Y_i-\bar Y_i + \bar Y_i -\hat f(X_i))^2|X_I] =  \\ = &
E[(Y_i-\bar Y_i)^2+ ( \bar Y_i -\hat f(X_i))^2|X_I] =  \\ = &
E[(Y_i-\bar Y_i)^2|X_I] 
+ E[(\bar Y_i -\hat f(X_i))^2]
\end{align*}

Of the two last summands, the first does not deppend on \(\hat f\) and
the seond is clearly minimized when \(\hat{f}(X_i)=E[Y_i|X_i]\), as we
wanted to prove. Note that we have used the property that for two
intependent random variables
\(E[(X-Y)^2] = E[(\bar X -X)^2)+ E[(\bar X - Y)^2]\)

    \textbf{b.} (10pt.) Show that the expected generalization error,
\(\mathcal{R} = E[(Y_i-\hat{f}(X_i))^2]\), can be decomposed into an
irreducible error, a bias term, and a variance term. Furthermore,
describe in plain English the meaning of variance and bias in this
context. (Again, it suffices to do this pointwise, conditionally on
\(X\)).

    Using the previous result, we have:

\begin{align*}
E[(Y_i-f(X_i))^2] =  &
E[(Y_i-\bar Y_i)^2] 
+ E[(\bar Y_i -\hat f(X_i))^2] = \\ = &
E[(Y_i-\bar Y_i)^2] 
+ E[(\bar Y_i -E[\hat f(X_i)]+E[\hat f(X_i)]-\hat f(X_i))^2] = \\ = &
E[(Y_i-\bar Y_i)^2] 
+ E[(\bar Y_i -E[\hat f(X_i)])^2+(E[\hat f(X_i)]-\hat f(X_i))^2] = \\ = &
E[(Y_i-\bar Y_i)^2] 
+ E[(\bar Y_i -E[\hat f(X_i)])^2]+E[(E[\hat f(X_i)]-\hat f(X_i))^2]
\end{align*}

Where we have used (again) the same property of the expectations of
squares of differences. THe first term is the irreducible error, the
second therm is the bias term and the third term the variance term.

The variance term is the ineherent variance on the predictor function
due to the fact that it is produced from random variables, the bias term
is the perdiction error inherent to the learning algorithm, due to the
fact that the real function cannot be totally understood by the
predictor in some cases. It cannot be solved even by adding more data.

    \textbf{c} (2pt.) In general how does a learning algorithm's flexibility
relate to the bias and variance terms described above?

    Assymptotically the flexibility decreases the bias, as since the
function space is bigger, there is in some sense a higher chance that
the space contains a function that is very similar to the real function
underlying the process. On the other hand, higher flexibility means
there are functions in the function space that charactherize the noise
as well (in the sense that they overfit), and therefore we expect the
variance to be higher. This can be formalized by saying that if we have
two function spaces \(E_1 \subseteq E_2\), then for any function we try
to approximate, the bias error of \(E_2\) will allways be smaller (or
equal) to the one for \(E_2\).

    \section{Question 4}\label{question-4}

Cross Validation for Polynomial Regression. (18 points) Consider the
following four data generating processes:

\begin{itemize}
\tightlist
\item
  DGP 1:
  \(Y = -2* 1_{\{X < -3\}} + 2.55* 1_{\{ X > -2\}} - 2* 1_{\{X>0\}} + 4* 1_{\{X > 2\}} -1* 1_{\{ X > 3\}}+ \epsilon\)
\item
  DGP 2: \(Y = 6 + 0.4 X - 0.36X^2 + 0.005 X^3 + \epsilon\)
\item
  DGP 3: \$Y = 2.83 * \sin(\frac{\pi}{2} \times X) +\epsilon \$
\item
  DGP 4: \(Y = 4 * \sin(3 \pi \times X) * 1_{\{X>0\}}+ \epsilon\)
\end{itemize}

\(X\) is drawn from the uniform distribution in {[}-4,4{]} and
\(\epsilon\) is drawn from a standard normal (\(\mu =0\), \(\sigma^2\) =
1).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} DGP1 \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZob{}}\PY{p}{(}
             \PY{l+m}{\PYZhy{}2}   \PY{o}{*}\PY{p}{(}X \PY{o}{\PYZlt{}} \PY{l+m}{\PYZhy{}3}\PY{p}{)}
             \PY{l+m}{+2.55}\PY{o}{*}\PY{p}{(}X \PY{o}{\PYZgt{}} \PY{l+m}{\PYZhy{}2}\PY{p}{)}
             \PY{l+m}{\PYZhy{}2}   \PY{o}{*}\PY{p}{(}X \PY{o}{\PYZgt{}}  \PY{l+m}{0}\PY{p}{)}
             \PY{l+m}{+4}   \PY{o}{*}\PY{p}{(}X \PY{o}{\PYZgt{}}  \PY{l+m}{2}\PY{p}{)}
             \PY{l+m}{\PYZhy{}1}   \PY{o}{*}\PY{p}{(}X \PY{o}{\PYZgt{}}  \PY{l+m}{3}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
         DGP2 \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZob{}}
             \PY{l+m}{6}\PY{l+m}{+0.4}\PY{o}{*}X\PY{l+m}{\PYZhy{}0.36}\PY{o}{*}X\PY{o}{\PYZca{}}\PY{l+m}{2}\PY{l+m}{+0.005}\PY{o}{*}X\PY{o}{\PYZca{}}\PY{l+m}{3}
         \PY{p}{\PYZcb{}}
         DGP3 \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZob{}}
             \PY{l+m}{2.83}\PY{o}{*}\PY{k+kp}{sin}\PY{p}{(}\PY{k+kc}{pi}\PY{o}{/}\PY{l+m}{2}\PY{o}{*}X\PY{p}{)}
         \PY{p}{\PYZcb{}}
         DGP4 \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZob{}}
             \PY{l+m}{4}\PY{o}{*}\PY{k+kp}{sin}\PY{p}{(}\PY{l+m}{3}\PY{o}{*}\PY{k+kc}{pi}\PY{o}{*}X\PY{p}{)}\PY{o}{*}\PY{p}{(}X\PY{o}{\PYZgt{}}\PY{l+m}{0}\PY{p}{)}
         \PY{p}{\PYZcb{}}
         ERR \PY{o}{=} rnorm
         DGP \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}DGP1\PY{p}{,}DGP2\PY{p}{,}DGP3\PY{p}{,}DGP4\PY{p}{)}
\end{Verbatim}

    (5 pts.) Write a function to estimate the generalization error of a
polynomial by \(k\)-fold cross-validation. It should take as arguments
the data, the degree of the polynomial, and the number of folds \(k\).
It should return the cross-validation mean squared error.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} genError \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}df\PY{p}{,}deg\PY{p}{,}k\PY{p}{)}\PY{p}{\PYZob{}}
             n \PY{o}{=} \PY{k+kp}{nrow}\PY{p}{(}df\PY{p}{)}\PY{p}{;}
             sampler \PY{o}{=} \PY{k+kp}{sample}\PY{p}{(}n\PY{p}{)}
             df \PY{o}{=} df\PY{p}{[}sampler\PY{p}{,}\PY{p}{]}
             folds \PY{o}{=} \PY{k+kp}{cut}\PY{p}{(}\PY{k+kp}{seq}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}n\PY{p}{)}\PY{p}{,}breaks\PY{o}{=}k\PY{p}{,}labels\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
             sse \PY{o}{=} \PY{l+m}{0}\PY{p}{;}
             ise \PY{o}{=} \PY{l+m}{0}\PY{p}{;}
             \PY{k+kr}{for}\PY{p}{(}i \PY{k+kr}{in} \PY{l+m}{1}\PY{o}{:}k\PY{p}{)}
             \PY{p}{\PYZob{}}
                 ind \PY{o}{=} \PY{k+kp}{which}\PY{p}{(}folds\PY{o}{==}i\PY{p}{,}arr.ind\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
                 test \PY{o}{=} df\PY{p}{[}ind\PY{p}{,}\PY{p}{]}
                 train \PY{o}{=} df\PY{p}{[}\PY{o}{\PYZhy{}}ind\PY{p}{,}\PY{p}{]}
                 model \PY{o}{=} lm\PY{p}{(}Y \PY{o}{\PYZti{}} poly\PY{p}{(}X\PY{p}{,}deg\PY{p}{)}\PY{p}{,} data \PY{o}{=} train\PY{p}{)}
                 
                 newY \PY{o}{=}  predict\PY{p}{(}model\PY{p}{,}test\PY{p}{)}
                 sse \PY{o}{=} sse \PY{o}{+} \PY{k+kp}{sum}\PY{p}{(}\PY{p}{(}newY\PY{o}{\PYZhy{}}test\PY{o}{\PYZdl{}}Y\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{2}\PY{p}{)}
                 newY \PY{o}{=}  predict\PY{p}{(}model\PY{p}{,}train\PY{p}{)}
                 ise \PY{o}{=} ise \PY{o}{+} \PY{k+kp}{sum}\PY{p}{(}\PY{p}{(}newY\PY{o}{\PYZhy{}}train\PY{o}{\PYZdl{}}Y\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m}{2}\PY{p}{)}
             \PY{p}{\PYZcb{}}
             \PY{k+kr}{return}\PY{p}{(}\PY{k+kt}{list}\PY{p}{(}sse\PY{o}{/}n\PY{p}{,}ise\PY{o}{/}\PY{p}{(}n\PY{o}{*}\PY{p}{(}k\PY{l+m}{\PYZhy{}1}\PY{p}{)}\PY{p}{)}\PY{p}{,}model\PY{p}{)}\PY{p}{)}
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    Now a function to make some plots using this!

    \textit{(5 pts.)} Generate a dataset of size \(n = 100\) from each of
the four DGPs. For OLS fits of polynomials of orders
\(d = 0 \ldots 10\), calculate the in-sample MSE and the cross-validated
MSE (with \(k=10\)). For each dataset, plot the in-sample mean squared
error and the cross-validated MSE as a function of \(d\). For each
dataset, what order polynomial has the best out of sample performance?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} N \PY{o}{=} \PY{l+m}{100}
         X \PY{o}{=} \PY{l+m}{8}\PY{o}{*}runif\PY{p}{(}N\PY{p}{)}\PY{l+m}{\PYZhy{}4}
         R \PY{o}{=} \PY{k+kp}{sapply}\PY{p}{(}DGP\PY{p}{,}\PY{k+kr}{function}\PY{p}{(}y\PY{p}{)}\PY{p}{\PYZob{}}y\PY{p}{(}X\PY{p}{)}\PY{o}{+}ERR\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
         df \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}R\PY{p}{)}
         \PY{k+kp}{colnames}\PY{p}{(}df\PY{p}{)} \PY{o}{=} \PY{l+m}{1}\PY{o}{:}\PY{l+m}{4}
         df\PY{o}{\PYZdl{}}x \PY{o}{=} X
         make\PYZus{}the\PYZus{}plot\PY{o}{=} \PY{k+kr}{function} \PY{p}{(}i\PY{p}{)}\PY{p}{\PYZob{}}
             helper \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}deg\PY{p}{)}\PY{p}{\PYZob{}}
                 d \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}df\PY{o}{\PYZdl{}}x\PY{p}{,}df\PY{p}{[}i\PY{p}{]}\PY{p}{)}
                 \PY{k+kp}{colnames}\PY{p}{(}d\PY{p}{)} \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{X\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Y\PYZsq{}}\PY{p}{)}
                 genError\PY{p}{(}d\PY{p}{,}deg\PY{p}{,}\PY{l+m}{4}\PY{p}{)}
             \PY{p}{\PYZcb{}}
         
             res \PY{o}{=} \PY{k+kp}{sapply}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}\PY{p}{,}helper\PY{p}{)}
             mse \PY{o}{=} \PY{k+kp}{unlist}\PY{p}{(}res\PY{p}{[}\PY{l+m}{1}\PY{p}{,}\PY{p}{]}\PY{p}{)}
             ise \PY{o}{=} \PY{k+kp}{unlist}\PY{p}{(}res\PY{p}{[}\PY{l+m}{2}\PY{p}{,}\PY{p}{]}\PY{p}{)}
             deg \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}
         
         
             results \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}deg\PY{p}{,}mse\PY{p}{,}ise\PY{p}{)}
             \PY{k+kp}{names}\PY{p}{(}results\PY{p}{)} \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{degree\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Out of sample\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{In sample\PYZsq{}}\PY{p}{)}
             results2 \PY{o}{=} results
         
             df \PY{o}{=} melt\PY{p}{(}results \PY{p}{,}  id.vars \PY{o}{=} \PY{l+s}{\PYZsq{}}\PY{l+s}{degree\PYZsq{}}\PY{p}{)}
             \PY{k+kp}{names}\PY{p}{(}df\PY{p}{)} \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Degree\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Error\PYZus{}type\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{MSE\PYZsq{}}\PY{p}{)}
         
         
             plot \PY{o}{=} ggplot\PY{p}{(}df\PY{p}{,} aes\PY{p}{(}x \PY{o}{=} Degree\PY{p}{,}y \PY{o}{=} MSE\PY{p}{)}\PY{p}{)}
             plot \PY{o}{=} plot \PY{o}{+} geom\PYZus{}line\PY{p}{(}aes\PY{p}{(}colour \PY{o}{=} Error\PYZus{}type\PY{p}{)}\PY{p}{)}
             plot \PY{o}{=} plot \PY{o}{+} ggtitle\PY{p}{(}\PY{k+kp}{paste}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Function DGP\PYZdq{}}\PY{p}{,}\PY{k+kp}{toString}\PY{p}{(}i\PY{p}{)}\PY{p}{,}sep\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{k+kt}{list}\PY{p}{(}results\PY{p}{,}\PY{k+kp}{which.min}\PY{p}{(}results\PY{p}{[[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Out of sample\PYZsq{}}\PY{p}{]]}\PY{p}{)}\PY{p}{,}plot\PY{p}{)}
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} x1 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{1}\PY{p}{)}
         x1\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         x1\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
         
         x2 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{2}\PY{p}{)}
         x2\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         x2\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
         
         x3 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{3}\PY{p}{)}
         x3\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         x3\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
         
         x4 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{4}\PY{p}{)}
         x4\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         x4\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}
\end{Verbatim}

    
    
    8

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_68_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    2

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_68_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    7

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_68_9.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    2

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_68_11.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kt}{c}\PY{p}{(}x1\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x2\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x3\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x4\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{)}
\end{Verbatim}

    Now if we repeat again for the case of \(10^4\) we obtain:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} N \PY{o}{=} \PY{l+m}{10000}
         X \PY{o}{=} \PY{l+m}{8}\PY{o}{*}runif\PY{p}{(}N\PY{p}{)}\PY{l+m}{\PYZhy{}4}
         R \PY{o}{=} \PY{k+kp}{sapply}\PY{p}{(}DGP\PY{p}{,}\PY{k+kr}{function}\PY{p}{(}y\PY{p}{)}\PY{p}{\PYZob{}}y\PY{p}{(}X\PY{p}{)}\PY{o}{+}ERR\PY{p}{(}X\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
         df \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}R\PY{p}{)}
         \PY{k+kp}{colnames}\PY{p}{(}df\PY{p}{)} \PY{o}{=} \PY{l+m}{1}\PY{o}{:}\PY{l+m}{4}
         df\PY{o}{\PYZdl{}}x \PY{o}{=} X
         x1 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{1}\PY{p}{)}
         x1\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         
         x2 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{2}\PY{p}{)}
         x2\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         
         x3 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{3}\PY{p}{)}
         x3\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
         
         x4 \PY{o}{=} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{4}\PY{p}{)}
         x4\PY{p}{[[}\PY{l+m}{3}\PY{p}{]]}
\end{Verbatim}

    
    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_71_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_71_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_71_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_71_7.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kt}{c}\PY{p}{(}x1\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x2\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x3\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{,}x4\PY{p}{[[}\PY{l+m}{2}\PY{p}{]]}\PY{p}{)}
\end{Verbatim}

    \begin{enumerate*}
\item 10
\item 4
\item 9
\item 9
\end{enumerate*}

    
    Extra: How does the out of sample error improve with the data for every
degree?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} testCompare\PY{o}{=} \PY{k+kr}{function} \PY{p}{(}interval\PYZus{}d\PY{p}{,}f\PY{p}{,}interval\PYZus{}i\PY{p}{,}n\PYZus{}times\PY{o}{=}\PY{l+m}{3}\PY{p}{)}\PY{p}{\PYZob{}}
             h \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}N\PY{p}{,}D\PY{p}{)}\PY{p}{\PYZob{}}
                 r \PY{o}{=} \PY{l+m}{0}\PY{p}{;}
                 n \PY{o}{=} \PY{k+kp}{round}\PY{p}{(}\PY{k+kp}{max}\PY{p}{(}n\PYZus{}times\PY{p}{,}\PY{p}{(}n\PYZus{}times\PY{o}{*}\PY{l+m}{1E2}\PY{p}{)}\PY{o}{/}N\PY{l+m}{+1}\PY{p}{)}\PY{p}{)}
                 \PY{k+kr}{for}\PY{p}{(}i \PY{k+kr}{in} \PY{l+m}{1}\PY{o}{:}n\PY{p}{)}\PY{p}{\PYZob{}}
                     X \PY{o}{=} \PY{l+m}{8}\PY{o}{*}runif\PY{p}{(}N\PY{p}{)}\PY{l+m}{\PYZhy{}4}
                     Y \PY{o}{=} f\PY{p}{(}X\PY{p}{)}\PY{o}{+}ERR\PY{p}{(}X\PY{p}{)}
                     df \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}X\PY{p}{,}Y\PY{p}{)}
                     r\PY{o}{=}r\PY{o}{+}genError\PY{p}{(}df\PY{p}{,}D\PY{p}{,}\PY{l+m}{4}\PY{p}{)}\PY{p}{[[}\PY{l+m}{1}\PY{p}{]]}
                 \PY{p}{\PYZcb{}}
                 r\PY{o}{/}n
             \PY{p}{\PYZcb{}}
             r \PY{o}{=} \PY{k+kp}{sapply}\PY{p}{(}interval\PYZus{}d\PY{p}{,}
                    \PY{k+kr}{function}\PY{p}{(}D\PY{p}{)}\PY{p}{\PYZob{}}
                        h2 \PY{o}{=} \PY{k+kr}{function}\PY{p}{(}N\PY{p}{)}\PY{p}{\PYZob{}}h\PY{p}{(}N\PY{p}{,}D\PY{p}{)}\PY{p}{\PYZcb{}}
                        \PY{k+kp}{sapply}\PY{p}{(}interval\PYZus{}i\PY{p}{,}
                               h2\PY{p}{)}
                    \PY{p}{\PYZcb{}}
                   \PY{p}{)}
             r \PY{o}{=} \PY{k+kt}{data.frame}\PY{p}{(}r\PY{p}{)}
             \PY{k+kp}{colnames}\PY{p}{(}r\PY{p}{)} \PY{o}{=} \PY{k+kp}{lapply}\PY{p}{(}interval\PYZus{}d\PY{p}{,} 
                            \PY{k+kp}{toString}\PY{p}{)}
             r\PY{o}{\PYZdl{}}x \PY{o}{=} interval\PYZus{}i
             r
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} make\PYZus{}the\PYZus{}plot\PY{o}{=}\PY{k+kr}{function}\PY{p}{(}i\PY{p}{)}\PY{p}{\PYZob{}}
             degrees \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}
             x \PY{o}{=} \PY{k+kp}{round}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{20}\PY{o}{*}\PY{l+m}{100}\PY{o}{\PYZca{}}\PY{p}{(}\PY{p}{(}\PY{l+m}{0}\PY{o}{:}\PY{l+m}{20}\PY{p}{)}\PY{o}{/}\PY{l+m}{20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             r \PY{o}{=} testCompare\PY{p}{(}degrees\PY{p}{,}DGP\PY{p}{[[}i\PY{p}{]]}\PY{p}{,}x\PY{p}{)}
             df \PY{o}{=} melt\PY{p}{(}r \PY{p}{,}  id.vars \PY{o}{=} \PY{l+s}{\PYZsq{}}\PY{l+s}{x\PYZsq{}}\PY{p}{)}
             \PY{k+kp}{names}\PY{p}{(}df\PY{p}{)} \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Iterations\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Degree\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{MSE\PYZsq{}}\PY{p}{)}
             plot \PY{o}{=} ggplot\PY{p}{(}df\PY{p}{,} aes\PY{p}{(}Iterations\PY{p}{,}MSE\PY{p}{)}\PY{p}{)}
             plot \PY{o}{=} plot \PY{o}{+} geom\PYZus{}line\PY{p}{(}aes\PY{p}{(}colour \PY{o}{=} Degree\PY{p}{)}\PY{p}{)}
             plot \PY{o}{=} plot \PY{o}{+} scale\PYZus{}y\PYZus{}log10\PY{p}{(}\PY{p}{)} \PY{o}{+} scale\PYZus{}x\PYZus{}log10\PY{p}{(}\PY{p}{)} 
             plot \PY{o}{=} plot \PY{o}{+} ggtitle\PY{p}{(}\PY{k+kp}{paste}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Equation DGP\PYZdq{}}\PY{p}{,}\PY{k+kp}{toString}\PY{p}{(}i\PY{p}{)}\PY{p}{,}sep\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             plot
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{1}\PY{p}{)}
\end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_76_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{2}\PY{p}{)}
\end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_77_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{3}\PY{p}{)}
\end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_78_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} make\PYZus{}the\PYZus{}plot\PY{p}{(}\PY{l+m}{4}\PY{p}{)}
\end{Verbatim}

    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Pset3_files/Pset3_79_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
