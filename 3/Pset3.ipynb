{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Question-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Question 1</a></div><div class=\"lev1\"><a href=\"#Question-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Question 2</a></div><div class=\"lev1\"><a href=\"#Question-4\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Question 4</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: reshape\n",
      "Loading required package: ggplot2\n",
      "Loading required package: MASS\n"
     ]
    }
   ],
   "source": [
    "require(reshape)\n",
    "require(ggplot2)\n",
    "require(MASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the inestimable help of Alexander Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "(25pt + 10pt) In this question, we use the file `brader.csv` which contains data from Brader, Valentino and Suhay (2008). The file includes the following variables for $n=265$ observations:\n",
    "\n",
    "* the outcome of interest -- a four-point scale in response to *Do you think the number of immigrants from foreign countries should be increased or decreased?*\n",
    "* tone of the story treatment (positive or negative)\n",
    "* ethnicity of the featured immigrant treatment (Mexican or Russian)\n",
    "* respondents' age\n",
    "* respondents' income\n",
    "\n",
    "\n",
    "Consider the following ordered logit model for an ordered outcome variable with four levels:\n",
    "        $$ \\Pr(Y_i \\leq j \\mid X_i) \\ = \\ \\frac{\\exp(\\psi_j - X_i^\\top\\beta)}\n",
    "            {1 + \\exp(\\psi_j - X_i^\\top\\beta)} $$\n",
    "for $j = 1,2,3,4$ and $i = 1,...,n$ where $\\psi_4=\\infty$ and $X_i = [{\\tt tone}_i \\ {\\tt eth}_i \\ {\\tt ppage}_i \\ {\\tt ppincimp}_i]^\\top$ (i.e. no intercept).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**a) (5pt)** Write down the likelihood function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To simplify the notation, the indexs* $i,j,k$ *are used consistently,* $i$ *to iterate over the number of observations,*$j$ *to iterate over the number of outcomes, and* $k$ * to iterate over the predictors.*\n",
    "\n",
    "The log-likelihood can be easily seen to be:\n",
    "\n",
    "$$\n",
    "l = \n",
    "\\prod_{i=1}^n \n",
    "\\frac{\\exp(\\psi_{Y_i} - X_i^\\top\\beta)}\n",
    "     {1 + \\exp(\\psi_{Y_i} - X_i^\\top\\beta)} -\n",
    "\\frac{\\exp(\\psi_{Y_i-1} - X_i^\\top\\beta)}\n",
    "     {1 + \\exp(\\psi_{Y_i-1} - X_i^\\top\\beta)}\n",
    "$$\n",
    "\n",
    "To simplify, from here on $\\phi$ will be the inverse link function and $\\phi'$ its derivative. This allows us to write the above expression in a more Matricial form as:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^n \\sum_{j=1}^4\n",
    "\\tilde M_{i,j}\n",
    "\\phi\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_k\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\tilde M = MK =  M \n",
    "\\begin{pmatrix}\n",
    "  1 &  0 &  0 &  0\\\\\n",
    " -1 &  1 &  0 &  0\\\\\n",
    "  0 & -1 &  1 &  0\\\\\n",
    "  0 &  0 & -1 &  1\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And $M$ is the OneHot matrix for the $Y_i$, i.e. a matrix that has $n$ rows, and each row is the row vector with zeros everywhere but at the position $j$, where the the observed $Y$ is $Y_j$.  \n",
    "Below, there is a function that computes $\\tilde M$ in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_M = function (Y,m){\n",
    "    \n",
    "    #This is just a One Hot encoder:\n",
    "    n = length(Y)\n",
    "    M1 = t(matrix(rep(c(1:m),n),m))\n",
    "    M2 = matrix(rep(c(Y),m),n)\n",
    "    M = (M1==M2)+0\n",
    "    \n",
    "    #Create the matrix K\n",
    "    K = diag(m+1)\n",
    "    K = -K[1:m,]+K[2:(m+1),]\n",
    "    K = K[1:m,1:m+1]\n",
    "    \n",
    "    #Return the product\n",
    "    M%*%K\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, however, we are interested in the log-likelihood, which is:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^n \\log \\sum_{j=1}^4\n",
    "\\tilde M_{i,j}\n",
    "\\phi\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_k\\right)\n",
    "$$\n",
    "\n",
    "The notation is further simplified if we pre-comute the $\\phi\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_k\\right)$ as $\\phi_{ij}'$ (and the same for $\\phi_{ij}'$.\n",
    "\n",
    "Below there is a wrapper in `R` that automatically defines these quantities for our function, so that we can use them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrapper = function(expression){\n",
    "    function(X,Y,beta,psi,\n",
    "             phi = function(x){exp(x)/(1+exp(x))},\n",
    "             dphi = function(x){exp(x)/(1+exp(x))**2})\n",
    "    {\n",
    "        psi = append(psi,10); #add the 'infinity'\n",
    "        m = length(psi); #number of outcomes\n",
    "        n = length(Y) #number of observatons\n",
    "        M = compute_M(Y,m);\n",
    "        \n",
    "        # transform beta_j to beta_jk (as in part d)\n",
    "        # whenever necessary\n",
    "        if(length(beta) == ncol(X))\n",
    "            beta2 = matrix(rep(beta,m),ncol=m,byrow=0)\n",
    "        else beta2 = beta\n",
    "            \n",
    "        #Compute the linear combination inside `psi`\n",
    "        linear =  - (X%*%beta2)\n",
    "        origin =  matrix(rep(psi,n) ,nrow = n, byrow = 1)\n",
    "        l = origin +  linear\n",
    "        \n",
    "         phix = phi(l);   phix[,m]=1; #impose the 'infinity'\n",
    "        dphix = phi(l);  dphix[,m]=0; #impose the 'infinity'\n",
    "\n",
    "        expression(X,Y,beta,psi,M,phix,dphix);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the log-likelihood function immediately using the abstraction wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood = wrapper(\n",
    "    function(X,Y,beta,psi,M,phi,dphi){\n",
    "        sum(log(rowSums(M*phi)))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper function is just an abstaction (see it at the end of the file, in the annex), that computes the variables `M`, `phi`$=\\phi\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_k\\right)$, `dphi`$=\\phi'\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_k\\right)$. We use it because everything is easier in those variables. When a function goes trough the \"wrapper\", its arguments change to become (X,Y,beta,psi), and the wrapper computes M, phi and dphi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (10pt) Derive the score functions for $\\beta$ and $\\psi_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To simplify the computations, let $\\phi_{ij}=\\phi(\\psi_{j} - X_i^\\top\\beta)$, and $\\phi_{ij}'=\\phi'(\\psi_{j} - X_i^\\top\\beta)$. Then we will have:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^n \\log \\sum_{j=1}^4\n",
    "\\tilde M_{i,j}\n",
    "\\phi_{i,j}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since deriveatives and sums commute freely, we can compute the score easily:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_k} = -\n",
    "\\sum_{i=1}^n \\left (\\sum_{j=0}^4 \\tilde M_{i,j}\\phi_{i,j} \\right )^{-1}\n",
    "\\left (\\sum_{j=0}^4\n",
    "\\tilde M_{i,j}\\phi_{i,j}'\\right )  X_{ik}\n",
    "$$\n",
    "\n",
    "in `R` that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_score =  wrapper(\n",
    "    function(X,Y,beta,psi,M,phi,dphi){\n",
    "        -(rowSums(M*dphi)/rowSums(M*phi))%*%X\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $\\psi$, the result is equally immediate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\psi_j} = \n",
    "\\sum_{i=1}^n \\left (\\sum_{j=0}^4 \\tilde M_{i,j}\\phi_{i,j} \\right )^{-1}\n",
    "\\tilde M_{i,j}\\phi_{i,j}'\n",
    "$$\n",
    "\n",
    "which in `R` can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "psi_score =  wrapper(\n",
    "    function(X,Y,beta,psi,M,phi,dphi){\n",
    "        r = (rowSums(M*phi)**-1%*%(M*dphi))\n",
    "        r[1:(length(r)-1)]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10pt) Using (a) and (b), calculate the maximum likelihood estimates of $\\beta$ and $\\psi_j$ and their standard errors via the `optim` function in R. Confirm your results by comparing them to outputs from the `polr` function in the `MASS` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brader = read.csv('Data/brader.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a Handler function for `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_style": "split",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likelihood_handler = function (x,data){\n",
    "    X = data[[1]]\n",
    "    Y = data[[2]]\n",
    "    o = ncol(X)\n",
    "    beta = x[1:o]\n",
    "    psi = x[(o+1):length(x)]\n",
    "    l = length(psi)\n",
    "    diff_psi = psi[2:l]-psi[1:(l-1)]\n",
    "    if(min(diff_psi)<=0)\n",
    "        return (1E5)\n",
    "        \n",
    "        \n",
    "    return(-log_likelihood(X,Y,beta,psi))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_style": "split",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_handler = function(x,data){\n",
    "    X = data[[1]]\n",
    "    Y = data[[2]]\n",
    "    o = ncol(X)\n",
    "    beta = x[1:o]\n",
    "    psi = x[(o+1):length(x)]\n",
    "    l = length(psi)\n",
    "    diff_psi = psi[2:l]-psi[1:(l-1)]\n",
    "    if(min(diff_psi)<=0)\n",
    "        return(x*0);\n",
    "    sbeta = (-beta_score(X,Y,beta,psi)) \n",
    "    spsi  = (-psi_score(X,Y,beta,psi)) \n",
    "    return(append(sbeta,spsi))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we prepare the data as our function needs it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = brader$immigr\n",
    "X = data.matrix(brader)[,2:5]\n",
    "data = list(X,Y)\n",
    "beta = c(0.5, 0, 0, 0)\n",
    "psi = c(-1, 0, 1)\n",
    "betapsi = append(beta,psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.75057472585271</li>\n",
       "\t<li>0.182514633025783</li>\n",
       "\t<li>0.00954246157357417</li>\n",
       "\t<li>0.00453801085115179</li>\n",
       "\t<li>-1.64987523554466</li>\n",
       "\t<li>0.153712692971585</li>\n",
       "\t<li>1.37459144576628</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.75057472585271\n",
       "\\item 0.182514633025783\n",
       "\\item 0.00954246157357417\n",
       "\\item 0.00453801085115179\n",
       "\\item -1.64987523554466\n",
       "\\item 0.153712692971585\n",
       "\\item 1.37459144576628\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.75057472585271\n",
       "2. 0.182514633025783\n",
       "3. 0.00954246157357417\n",
       "4. 0.00453801085115179\n",
       "5. -1.64987523554466\n",
       "6. 0.153712692971585\n",
       "7. 1.37459144576628\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0.750574726  0.182514633  0.009542462  0.004538011 -1.649875236\n",
       "[6]  0.153712693  1.374591446"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "325.94791194349"
      ],
      "text/latex": [
       "325.94791194349"
      ],
      "text/markdown": [
       "325.94791194349"
      ],
      "text/plain": [
       "[1] 325.9479"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = optim(betapsi,likelihood_handler, gr = gradient_handler, data = data,\n",
    "          hessian = 1, control = (reltol = 1E-12))\n",
    "r$par\n",
    "r$value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "polr(formula = factor(immigr) ~ tone + eth + ppage + ppincimp, \n",
       "    data = brader, Hess = 1, method = \"logistic\")\n",
       "\n",
       "Coefficients:\n",
       "            Value Std. Error t value\n",
       "tone     0.749446   0.230241  3.2550\n",
       "eth      0.166225   0.227063  0.7321\n",
       "ppage    0.009345   0.007131  1.3105\n",
       "ppincimp 0.004149   0.029511  0.1406\n",
       "\n",
       "Intercepts:\n",
       "    Value   Std. Error t value\n",
       "1|2 -1.6671  0.5664    -2.9434\n",
       "2|3  0.1318  0.5401     0.2441\n",
       "3|4  1.3535  0.5466     2.4761\n",
       "\n",
       "Residual Deviance: 651.8892 \n",
       "AIC: 665.8892 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "325.944618379387"
      ],
      "text/latex": [
       "325.944618379387"
      ],
      "text/markdown": [
       "325.944618379387"
      ],
      "text/plain": [
       "[1] 325.9446"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "\n",
    "plr <- polr( factor(immigr) ~ tone + eth + ppage + ppincimp, \n",
    "                   data = brader, method = \"logistic\", Hess = 1)\n",
    "summary(plr)\n",
    "-log_likelihood(X,Y,plr$coefficients,plr$z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** d. (10pt)  Bonus question. ** The standard ordered logit model is sometimes called the  proportional odds model because it assumes the effect of $X_i$ is constant across levels on the odds ratio scale. One approach to relax this assumption is to allow the coefficients to vary across levels, i.e.,\n",
    "        $$ \\Pr(Y_i \\leq j \\mid X_i) \\ = \\ \\frac{\\exp(\\psi_j - X_i^\\top\\beta_j)}\n",
    "            {1 + \\exp(\\psi_j - X_i^\\top\\beta_j)} $$\n",
    "for $j = 1,2,3,4$ and $i = 1,...,n$ where $\\beta_4=0$ (i.e. the fourth group is a reference group), and $\\psi_4=\\infty$. For this model, derive the likelihood and score functions, and use `optim` to obtain the maximum likelihood estimates of $\\beta_j$ and $\\psi_j$ and their standard errors for the `brader` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct modification of the above exercise, substituting $\\beta_k$ for $\\beta_{k,j}$ works:\n",
    "We will have:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^n \\log \\sum_{j=1}^4\n",
    "\\tilde M_{i,j}\n",
    "\\phi\\left(\\psi_{j} - \\sum_{k=1}^m X_{ik}\\beta_{kj}\\right)\n",
    "$$\n",
    "\n",
    "therefore, since the formula is essentialy the same (and thanks to `R` not really caring much about the shape of the objects  -- which is nice!), the same `log_likelihood` coded above still works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since deriveatives and sums commute freely, we can compute the score easily:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_{kj}} = -\n",
    "\\sum_{i=1}^n \\left (\\sum_{j=0}^4 \\tilde M_{i,j}\\phi_{i,j} \\right )^{-1}\n",
    "\\tilde M_{i,j}\\phi_{i,j}'  X_{ik}\n",
    "$$\n",
    "\n",
    "in this case we must modify the function, to create a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_beta_score = wrapper(\n",
    "    function(X,Y,beta,psi,M,phi,dphi){\n",
    "        #this corresponds to a matrix that has the same elements in every\n",
    "        #row, and are the (sum **)^{-1} in the formula above\n",
    "        M1 = matrix(rep(rowSums(M*phi),ncol(M)),ncol=ncol(M),byrow=0)\n",
    "        -t(M*dphi/M1)%*%X\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $\\psi$, the result is equally immediate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\psi_j} = \n",
    "\\sum_{i=1}^n \\left (\\sum_{j=0}^4 \\tilde M_{i,j}\\phi_{i,j} \\right )^{-1}\n",
    "\\tilde M_{i,j}\\phi_{i,j}'\n",
    "$$\n",
    "\n",
    "As for the likelihood, the same function does the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(30 pt.) Understanding the basics of maximum likelihood makes it easier to learn a wide range of models. Here you will learn about a model not covered in class. The {\\it tobit model} is often used to model a censored\n",
    "outcome variable, where we only observe values of the outcome above a certain threshold $\\tau$ while values below that threshold take on a single value, say 0. A classic example is labor market participation:  you only observe a worker's wage if their potential earnings is high enough to be over minimium wage or otherwise employable.\n",
    "\n",
    "Consider the case where $\\tau=0$. In this case, the model can be written as follows.\n",
    "  \\begin{eqnarray*}\n",
    "    Y_i & = & \\left\\{ \\begin{array}{ll}\n",
    "                    Y_i^\\ast & {\\rm if} \\quad Y_i^\\ast > 0 \\\\\n",
    "                    0 & {\\rm if} \\quad Y_i^\\ast \\le 0 \\end{array} \\right.\n",
    "                    \\quad {\\rm where} \\quad Y_i^\\ast = X_i^\\top \\beta + \\epsilon_i,\n",
    "  \\end{eqnarray*}\n",
    "  where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.  Note that $Y_i$\n",
    "  is the observed outcome variable and $Y_i^\\ast$ is the underlying\n",
    "  latent variable, which is not (always) observable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.**(10pt) Derive the likelihood and log-likelihood functions of the model for a simple random sample of size $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will (for a moment) forget that log-likelihood is in fact terribly ill posed for variables that are a mixture of a continuous and a discrete variable, and proceed as if we didn't know that:\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\n",
    "P(x_i=0) = \\psi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Cross Validation for Polynomial Regression. (18 points)\n",
    "Consider the following four data generating processes:\n",
    "\n",
    "* DGP 1: $Y = -2* 1_{\\{X < -3\\}} + 2.55* 1_{\\{ X > -2\\}} - 2* 1_{\\{X>0\\}} + 4* 1_{\\{X > 2\\}} -1* 1_{\\{ X > 3\\}}+ \\epsilon$\n",
    "* DGP 2: $Y = 6 + 0.4 X - 0.36X^2 + 0.005 X^3 + \\epsilon$\n",
    "* DGP 3: $Y = 2.83 * \\sin(\\frac{\\pi}{2} \\times X) +\\epsilon $\n",
    "* DGP 4: $Y = 4 * \\sin(3 \\pi \\times X) * 1_{\\{X>0\\}}+ \\epsilon$\n",
    "\n",
    "  $X$ is drawn from the uniform distribution in [-4,4] and $\\epsilon\n",
    "  $\n",
    "  is drawn from a standard normal ($\\mu =0$, $\\sigma^2$ = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DGP1 = function(X){(\n",
    "    -2   *(X < -3)\n",
    "    +2.55*(X > -2)\n",
    "    -2   *(X >  0)\n",
    "    +4   *(X >  2)\n",
    "    -1   *(X >  3))}\n",
    "DGP2 = function(X){\n",
    "    6+0.4*X-0.36*X^2+0.005*X^3\n",
    "}\n",
    "DGP3 = function(X){\n",
    "    2.83*sin(pi/2*X)\n",
    "}\n",
    "DGP4 = function(X){\n",
    "    4*sin(3*pi*x)*(X>0)\n",
    "}\n",
    "ERR = rnorm\n",
    "DGP = c(DGP1,DGP2,DGP3,DGP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 pts.) Write a function to estimate the generalization error of a polynomial by $k$-fold cross-validation. It should take as arguments the data, the degree of the polynomial, and the number of folds $k$. It should return the cross-validation mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genError = function(df,deg,k){\n",
    "    n = nrow(df);\n",
    "    sampler = sample(n)\n",
    "    df = df[sampler,]\n",
    "    folds = cut(seq(1,n),breaks=k,labels=FALSE)\n",
    "    sse = 0;\n",
    "    for(i in 1:k)\n",
    "    {\n",
    "        ind = which(folds==i,arr.ind=TRUE)\n",
    "        test = df[ind,]\n",
    "        train = df[-ind,]\n",
    "        model = lm(Y ~ poly(X,deg), data = train)\n",
    "        newY =  predict(model,test)\n",
    "        sse = sse + sum((newY-test$Y)**2)\n",
    "    }\n",
    "    return(sse/n)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCompare= function (interval_d,f,interval_i,n_times=3){\n",
    "    h = function(N,D){\n",
    "        r = 0;\n",
    "        n = round(max(n_times,(n_times*1E2)/N+1))\n",
    "        for(i in 1:n){\n",
    "            X = 8*runif(N)-4\n",
    "            Y = f(X)+ERR(X)\n",
    "            df = data.frame(X,Y)\n",
    "            r=r+genError(df,D,4)\n",
    "        }\n",
    "        r/n\n",
    "    }\n",
    "    r = sapply(interval_d,\n",
    "           function(D){\n",
    "               h2 = function(N){h(N,D)}\n",
    "               sapply(interval_i,\n",
    "                      h2)\n",
    "           }\n",
    "          )\n",
    "    r = data.frame(r)\n",
    "    colnames(r) = lapply(interval_d, \n",
    "                   function(x){paste('',toString(x),sep='')})\n",
    "    r$x = interval_i\n",
    "    r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_the_plot=function(i){\n",
    "    degrees = c(1:5)\n",
    "    x = round(c(10*1.125^(2:20)))\n",
    "    r = testCompare(degrees,DGP[[i]],x)\n",
    "    df = melt(r ,  id.vars = 'x')\n",
    "    names(df) = c('Iterations', 'Degree','value')\n",
    "    plot = ggplot(df, aes(Iterations,value))\n",
    "    plot = plot + geom_line(aes(colour = Degree))\n",
    "    plot = plot + scale_y_log10() + scale_x_log10() \n",
    "    plot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in f(X): object 'x' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in f(X): object 'x' not found\nTraceback:\n",
      "1. make_the_plot(i)",
      "2. testCompare(degrees, DGP[[i]], x)   # at line 4 of file <text>",
      "3. sapply(interval_d, function(D) {\n .     h2 = function(N) {\n .         h(N, D)\n .     }\n .     sapply(interval_i, h2)\n . })   # at line 13-19 of file <text>",
      "4. lapply(X = X, FUN = FUN, ...)",
      "5. FUN(X[[i]], ...)",
      "6. sapply(interval_i, h2)   # at line 16-17 of file <text>",
      "7. lapply(X = X, FUN = FUN, ...)",
      "8. FUN(X[[i]], ...)",
      "9. h(N, D)   # at line 15 of file <text>",
      "10. f(X)   # at line 7 of file <text>"
     ]
    }
   ],
   "source": [
    "for(i in 1:4) make_the_plot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
